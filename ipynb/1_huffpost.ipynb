{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDTQwe8DR8i8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from langdetect import detect\n",
        "from collections import Counter\n",
        "\n",
        "# Download necessary resources for NLTK\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the news dataset\n",
        "def load_dataset(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        headlines = []\n",
        "        for line in file:\n",
        "            headlines.append(line.strip())\n",
        "    return headlines\n",
        "\n",
        "# Function to detect languages of headlines\n",
        "def language_detection(headlines):\n",
        "    languages = []\n",
        "    for headline in headlines:\n",
        "        detected_language = detect(headline)\n",
        "        languages.append(detected_language)\n",
        "    return languages\n",
        "\n",
        "# Function to count total words in headlines\n",
        "def word_count(headlines):\n",
        "    total_words = 0\n",
        "    for headline in headlines:\n",
        "        words = word_tokenize(headline)\n",
        "        total_words += len(words)\n",
        "    return total_words\n",
        "\n",
        "# Function to count total sentences in headlines\n",
        "def sentence_count(headlines):\n",
        "    total_sentences = 0\n",
        "    for headline in headlines:\n",
        "        sentences = sent_tokenize(headline)\n",
        "        total_sentences += len(sentences)\n",
        "    return total_sentences\n",
        "\n",
        "# Function to tokenize words in headlines\n",
        "def word_tokenization(headlines):\n",
        "    words_tokenized = []\n",
        "    for headline in headlines:\n",
        "        words = word_tokenize(headline)\n",
        "        words_tokenized.append(words)\n",
        "    return words_tokenized\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # file_path = \"news_dataset.txt\"\n",
        "    # headlines = load_dataset(file_path)\n",
        "    headlines = []\n",
        "    # Language detection\n",
        "    detected_languages = language_detection(headlines)\n",
        "    print(\"Detected Languages:\", detected_languages[:5])  # Displaying first 5 detected languages\n",
        "\n",
        "    # Word count\n",
        "    total_words = word_count(headlines)\n",
        "    print(\"Total Words:\", total_words)\n",
        "\n",
        "    # Sentence count\n",
        "    total_sentences = sentence_count(headlines)\n",
        "    print(\"Total Sentences:\", total_sentences)\n",
        "\n",
        "    # Word tokenization\n",
        "    words_tokenized = word_tokenization(headlines)\n",
        "    print(\"Words Tokenized:\", words_tokenized[:2])  # Displaying tokenized words of first 2 headlines\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, WhitespaceTokenizer, RegexpTokenizer, TweetTokenizer, PunktSentenceTokenizer, WordPunctTokenizer, TreebankWordTokenizer\n",
        "\n",
        "# Sample text for demonstration\n",
        "text = \"NLTK is a powerful library for natural language processing. It provides various tokenizers for different tasks, such as word tokenization and sentence tokenization. Tweet @NLTKOrg for more info! #NLTK #NLP\"\n",
        "\n",
        "# Word Tokenizer\n",
        "words_word_tokenize = word_tokenize(text)\n",
        "print(\"Word Tokenizer:\", words_word_tokenize)\n",
        "\n",
        "# Sentence Tokenizer\n",
        "sentences_sent_tokenize = sent_tokenize(text)\n",
        "print(\"Sentence Tokenizer:\", sentences_sent_tokenize)\n",
        "\n",
        "# Whitespace Tokenizer\n",
        "tokenizer_whitespace = WhitespaceTokenizer()\n",
        "words_whitespace_tokenize = tokenizer_whitespace.tokenize(text)\n",
        "print(\"Whitespace Tokenizer:\", words_whitespace_tokenize)\n",
        "\n",
        "# Regexp Tokenizer\n",
        "tokenizer_regexp = RegexpTokenizer(r'\\w+')\n",
        "words_regexp_tokenize = tokenizer_regexp.tokenize(text)\n",
        "print(\"Regexp Tokenizer:\", words_regexp_tokenize)\n",
        "\n",
        "# WordPunct Tokenizer\n",
        "tokenizer_wordpunct = WordPunctTokenizer()\n",
        "words_wordpunct_tokenize = tokenizer_wordpunct.tokenize(text)\n",
        "print(\"WordPunct Tokenizer:\", words_wordpunct_tokenize)\n",
        "\n"
      ]
    }
  ]
}