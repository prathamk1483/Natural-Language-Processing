{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect\n",
        "!pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JOzUYeJ_FbP",
        "outputId": "aee54196-8e53-41e5-b887-9e4ce2fbc8ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=4f8c7002d166af5baae3020fb4b2aaf2c678a2ce2e2c7f27d202c1da01eb78f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg7OasNH929t",
        "outputId": "7cffecca-edd1-468e-fab8-c516a19ed95a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Languages: []\n",
            "Total Words: 0\n",
            "Total Sentences: 0\n",
            "Words Tokenized: []\n",
            "Word Tokenizer: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', '.', 'It', 'provides', 'various', 'tokenizers', 'for', 'different', 'tasks', ',', 'such', 'as', 'word', 'tokenization', 'and', 'sentence', 'tokenization', '.', 'Tweet', '@', 'NLTKOrg', 'for', 'more', 'info', '!', '#', 'NLTK', '#', 'NLP']\n",
            "Sentence Tokenizer: ['NLTK is a powerful library for natural language processing.', 'It provides various tokenizers for different tasks, such as word tokenization and sentence tokenization.', 'Tweet @NLTKOrg for more info!', '#NLTK #NLP']\n",
            "Whitespace Tokenizer: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing.', 'It', 'provides', 'various', 'tokenizers', 'for', 'different', 'tasks,', 'such', 'as', 'word', 'tokenization', 'and', 'sentence', 'tokenization.', 'Tweet', '@NLTKOrg', 'for', 'more', 'info!', '#NLTK', '#NLP']\n",
            "Regexp Tokenizer: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', 'It', 'provides', 'various', 'tokenizers', 'for', 'different', 'tasks', 'such', 'as', 'word', 'tokenization', 'and', 'sentence', 'tokenization', 'Tweet', 'NLTKOrg', 'for', 'more', 'info', 'NLTK', 'NLP']\n",
            "WordPunct Tokenizer: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', '.', 'It', 'provides', 'various', 'tokenizers', 'for', 'different', 'tasks', ',', 'such', 'as', 'word', 'tokenization', 'and', 'sentence', 'tokenization', '.', 'Tweet', '@', 'NLTKOrg', 'for', 'more', 'info', '!', '#', 'NLTK', '#', 'NLP']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from langdetect import detect\n",
        "from collections import Counter\n",
        "\n",
        "# Download necessary resources for NLTK\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the news dataset\n",
        "def load_dataset(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        headlines = []\n",
        "        for line in file:\n",
        "            headlines.append(line.strip())\n",
        "    return headlines\n",
        "\n",
        "# Function to detect languages of headlines\n",
        "def language_detection(headlines):\n",
        "    languages = []\n",
        "    for headline in headlines:\n",
        "        detected_language = detect(headline)\n",
        "        languages.append(detected_language)\n",
        "    return languages\n",
        "\n",
        "# Function to count total words in headlines\n",
        "def word_count(headlines):\n",
        "    total_words = 0\n",
        "    for headline in headlines:\n",
        "        words = word_tokenize(headline)\n",
        "        total_words += len(words)\n",
        "    return total_words\n",
        "\n",
        "# Function to count total sentences in headlines\n",
        "def sentence_count(headlines):\n",
        "    total_sentences = 0\n",
        "    for headline in headlines:\n",
        "        sentences = sent_tokenize(headline)\n",
        "        total_sentences += len(sentences)\n",
        "    return total_sentences\n",
        "\n",
        "# Function to tokenize words in headlines\n",
        "def word_tokenization(headlines):\n",
        "    words_tokenized = []\n",
        "    for headline in headlines:\n",
        "        words = word_tokenize(headline)\n",
        "        words_tokenized.append(words)\n",
        "    return words_tokenized\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # file_path = \"news_dataset.txt\"\n",
        "    # headlines = load_dataset(file_path)\n",
        "    headlines = []\n",
        "    # Language detection\n",
        "    detected_languages = language_detection(headlines)\n",
        "    print(\"Detected Languages:\", detected_languages[:5])  # Displaying first 5 detected languages\n",
        "\n",
        "    # Word count\n",
        "    total_words = word_count(headlines)\n",
        "    print(\"Total Words:\", total_words)\n",
        "\n",
        "    # Sentence count\n",
        "    total_sentences = sentence_count(headlines)\n",
        "    print(\"Total Sentences:\", total_sentences)\n",
        "\n",
        "    # Word tokenization\n",
        "    words_tokenized = word_tokenization(headlines)\n",
        "    print(\"Words Tokenized:\", words_tokenized[:2])  # Displaying tokenized words of first 2 headlines\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, WhitespaceTokenizer, RegexpTokenizer, TweetTokenizer, PunktSentenceTokenizer, WordPunctTokenizer, TreebankWordTokenizer\n",
        "\n",
        "# Sample text for demonstration\n",
        "text = \"NLTK is a powerful library for natural language processing. It provides various tokenizers for different tasks, such as word tokenization and sentence tokenization. Tweet @NLTKOrg for more info! #NLTK #NLP\"\n",
        "\n",
        "# Word Tokenizer\n",
        "words_word_tokenize = word_tokenize(text)\n",
        "print(\"Word Tokenizer:\", words_word_tokenize)\n",
        "\n",
        "# Sentence Tokenizer\n",
        "sentences_sent_tokenize = sent_tokenize(text)\n",
        "print(\"Sentence Tokenizer:\", sentences_sent_tokenize)\n",
        "\n",
        "# Whitespace Tokenizer\n",
        "tokenizer_whitespace = WhitespaceTokenizer()\n",
        "words_whitespace_tokenize = tokenizer_whitespace.tokenize(text)\n",
        "print(\"Whitespace Tokenizer:\", words_whitespace_tokenize)\n",
        "\n",
        "# Regexp Tokenizer\n",
        "tokenizer_regexp = RegexpTokenizer(r'\\w+')\n",
        "words_regexp_tokenize = tokenizer_regexp.tokenize(text)\n",
        "print(\"Regexp Tokenizer:\", words_regexp_tokenize)\n",
        "\n",
        "# WordPunct Tokenizer\n",
        "tokenizer_wordpunct = WordPunctTokenizer()\n",
        "words_wordpunct_tokenize = tokenizer_wordpunct.tokenize(text)\n",
        "print(\"WordPunct Tokenizer:\", words_wordpunct_tokenize)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# Sample words for stemming\n",
        "words = [\"running\", \"ran\", \"runs\", \"runner\", \"easily\", \"fairly\", \"fairness\"]\n",
        "\n",
        "# Porter Stemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "stemmed_words_porter = [porter_stemmer.stem(word) for word in words]\n",
        "print(\"Porter Stemmer:\", stemmed_words_porter)\n",
        "\n",
        "# Lancaster Stemmer\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "stemmed_words_lancaster = [lancaster_stemmer.stem(word) for word in words]\n",
        "print(\"Lancaster Stemmer:\", stemmed_words_lancaster)\n",
        "\n",
        "\n",
        "# Snowball Stemmer for English\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "stemmed_words_snowball = [snowball_stemmer.stem(word) for word in words]\n",
        "print(\"Snowball Stemmer:\", stemmed_words_snowball)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7x7CHJkI-nfh",
        "outputId": "21e975e2-4a49-4586-d4d3-caf804e143a1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer: ['run', 'ran', 'run', 'runner', 'easili', 'fairli', 'fair']\n",
            "Lancaster Stemmer: ['run', 'ran', 'run', 'run', 'easy', 'fair', 'fair']\n",
            "Snowball Stemmer: ['run', 'ran', 'run', 'runner', 'easili', 'fair', 'fair']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text for demonstration\n",
        "text = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
        "\"\"\"\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Perform POS tagging and count POS tags\n",
        "pos_tags = Counter()\n",
        "for token in doc:\n",
        "    pos_tags[token.pos_] += 1\n",
        "\n",
        "# Print frequency list of POS tags\n",
        "print(\"Frequency of POS tags:\")\n",
        "for tag, count in pos_tags.items():\n",
        "    print(f\"{tag}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "230v9vaP-0Cl",
        "outputId": "324679a0-045d-40d2-bdbf-7a260e5911d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency of POS tags:\n",
            "SPACE: 2\n",
            "ADJ: 9\n",
            "NOUN: 23\n",
            "PUNCT: 9\n",
            "PROPN: 1\n",
            "AUX: 1\n",
            "DET: 2\n",
            "ADP: 6\n",
            "CCONJ: 4\n",
            "VERB: 5\n",
            "SCONJ: 1\n",
            "PART: 2\n",
            "ADV: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Sample text for demonstration\n",
        "text = \"I love this movie! It's so uplifting and heartwarming. However, the ending made me sad.\"\n",
        "\n",
        "# Sentiment analysis using TextBlob\n",
        "blob = TextBlob(text)\n",
        "sentiment = blob.sentiment.polarity\n",
        "sentiment_label = \"positive\" if sentiment > 0 else \"negative\"\n",
        "\n",
        "print(\"Sentiment (TextBlob):\", sentiment_label)\n",
        "\n",
        "# Emotion detection using NLTK\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "tokens = word_tokenize(text)\n",
        "emotion_scores = sia.polarity_scores(text)\n",
        "\n",
        "# Determine emotion based on the sentiment scores\n",
        "emotion_label = None\n",
        "if emotion_scores['pos'] > emotion_scores['neg']:\n",
        "    emotion_label = \"positive\"\n",
        "elif emotion_scores['pos'] < emotion_scores['neg']:\n",
        "    emotion_label = \"negative\"\n",
        "else:\n",
        "    emotion_label = \"neutral\"\n",
        "\n",
        "print(\"Emotion (NLTK):\", emotion_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaFTqf8I-0ZI",
        "outputId": "8bff1443-e864-4bd1-e085-09f0da8e4f09"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment (TextBlob): positive\n",
            "Emotion (NLTK): positive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "# Sample text for demonstration\n",
        "text = \"I love natural language processing. It is fascinating.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Define the order of the N-gram model (N)\n",
        "N = 2\n",
        "\n",
        "# Generate N-grams from the tokenized words\n",
        "ngrams_list = list(ngrams(words, N))\n",
        "\n",
        "# Count occurrences of each N-gram\n",
        "ngram_counts = Counter(ngrams_list)\n",
        "\n",
        "# Calculate the probability of a sequence of words using N-gram model\n",
        "def calculate_probability(sequence):\n",
        "    if len(sequence) != N:\n",
        "        raise ValueError(\"Length of sequence must be equal to N\")\n",
        "\n",
        "    # Convert the sequence to tuple for comparison with N-grams\n",
        "    sequence_tuple = tuple(sequence)\n",
        "\n",
        "    # Count occurrences of the sequence in the N-grams\n",
        "    sequence_count = ngram_counts.get(sequence_tuple, 0)\n",
        "\n",
        "    # Count occurrences of the first (N-1) words in the sequence\n",
        "    prefix_count = sum(1 for gram in ngrams_list if gram[:-1] == sequence_tuple[:-1])\n",
        "\n",
        "    # Calculate the probability using Laplace smoothing (add-one smoothing)\n",
        "    probability = (sequence_count + 1) / (prefix_count + len(set(ngrams_list)))\n",
        "\n",
        "    return probability\n",
        "\n",
        "# Example: Calculate the probability of a sequence \"natural language\"\n",
        "sequence = [\"natural\", \"language\"]\n",
        "probability = calculate_probability(sequence)\n",
        "print(f\"Probability of sequence '{' '.join(sequence)}': {probability}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxCjXAAv-_T_",
        "outputId": "32e2ceff-6bfa-4d6c-b6ec-ee1d0b8cf831"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of sequence 'natural language': 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text for NER\n",
        "text = \"\"\"\n",
        "Apple was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976.\n",
        "It is headquartered in Cupertino, California, United States.\n",
        "Apple designs, manufactures, and sells consumer electronics, computer software,\n",
        "and online services. Its best-known hardware products include the iPhone,\n",
        "iPad, Mac, and Apple Watch.\n",
        "\"\"\"\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract named entities\n",
        "named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "# Display named entities\n",
        "print(\"Named Entities:\")\n",
        "for entity, label in named_entities:\n",
        "    print(f\"{entity} - {label}\")\n",
        "\n",
        "# Visualize named entities\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "nKq8nafpA97t",
        "outputId": "8ad3d49e-9db6-4547-87d9-26f5f101abca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities:\n",
            "Apple - ORG\n",
            "Steve Jobs - PERSON\n",
            "Steve Wozniak - PERSON\n",
            "Ronald Wayne - PERSON\n",
            "1976 - DATE\n",
            "Cupertino - GPE\n",
            "California - GPE\n",
            "United States - GPE\n",
            "Apple - ORG\n",
            "iPhone - ORG\n",
            "iPad - ORG\n",
            "Mac - PERSON\n",
            "Apple Watch - ORG\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"><br>\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " was founded by \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Steve Jobs\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Steve Wozniak\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", and \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Ronald Wayne\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1976\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". <br>It is headquartered in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Cupertino\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    California\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    United States\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ". <br>\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " designs, manufactures, and sells consumer electronics, computer software, <br>and online services. Its best-known hardware products include the \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    iPhone\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", <br>\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    iPad\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Mac\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", and \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple Watch\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ".<br></div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the IMDb movie reviews dataset\n",
        "data = pd.read_csv(\"imdb_reviews.csv\")\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation, special characters, and numbers\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "data['cleaned_review'] = data['review'].apply(preprocess_text)\n",
        "\n",
        "# Feature Extraction\n",
        "X = data['cleaned_review']  # Input features (cleaned movie reviews)\n",
        "y = data['label']   # Target variable (sentiment label)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Convert text data into numerical representations\n",
        "X = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model Building\n",
        "model = LogisticRegression(max_iter=1000)  # Logistic Regression model\n",
        "model.fit(X_train, y_train)  # Train the model on the training set\n",
        "\n",
        "# Model Evaluation\n",
        "y_pred = model.predict(X_test)  # Predict the labels for the testing set\n",
        "accuracy = accuracy_score(y_test, y_pred)  # Compute the accuracy of the model\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEO6DMMvBJO2",
        "outputId": "e5249865-d7fc-493a-f568-f79ddaab1379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8375\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.85      0.83      0.84       204\n",
            "         pos       0.83      0.85      0.84       196\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the dataset containing spam and ham messages\n",
        "data = pd.read_csv(\"spam_ham_messages.csv\")\n",
        "\n",
        "# Data Preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation, special characters, and numbers\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "data['cleaned_message'] = data['message'].apply(preprocess_text)\n",
        "\n",
        "# Feature Extraction\n",
        "X = data['cleaned_message']  # Input features (cleaned messages)\n",
        "y = data['label']   # Target variable (spam or ham)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000)  # Convert text data into numerical representations\n",
        "X = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model Building\n",
        "model = LogisticRegression(max_iter=1000)  # Logistic Regression model\n",
        "model.fit(X_train, y_train)  # Train the model on the training set\n",
        "\n",
        "# Model Evaluation\n",
        "y_pred = model.predict(X_test)  # Predict the labels for the testing set\n",
        "accuracy = accuracy_score(y_test, y_pred)  # Compute the accuracy of the model\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "J_WPDEU9BR_J",
        "outputId": "0922b087-ca03-4f2e-f32b-8c015adf4eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'spam_ham_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-517ade35ba44>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the spam/ham dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spam_ham_dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Separate the text and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'spam_ham_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Train an HMM POS tagger on the Brown corpus\n",
        "nltk.download('brown')\n",
        "brown_corpus = nltk.corpus.brown.tagged_sents()\n",
        "hmm_tagger = nltk.HiddenMarkovModelTagger.train(brown_corpus)\n",
        "\n",
        "# Test the tagger on a sample sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "tagged_sentence = hmm_tagger.tag(nltk.word_tokenize(sentence))\n",
        "print(tagged_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0h_5KbUBWzg",
        "outputId": "efc3d591-aacf-44a4-e02c-704fbddeeac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'AT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'AT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "import gensim\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text for word embeddings\n",
        "text = \"I love natural language processing. It is fascinating.\"\n",
        "\n",
        "# Method 1: Google News Word2Vec\n",
        "print(\"Google News Word2Vec:\")\n",
        "try:\n",
        "    # Load Google News Word2Vec model (large model, download may take some time)\n",
        "    model_google_news = api.load('word2vec-google-news-300')\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Filter out words not in vocabulary\n",
        "    words_in_vocab = []\n",
        "    for word in words:\n",
        "        if word in model_google_news.vocab:\n",
        "            words_in_vocab.append(word)\n",
        "    # Generate word embeddings\n",
        "    word_embeddings_google_news = []\n",
        "    for word in words_in_vocab:\n",
        "        word_embeddings_google_news.append(model_google_news[word])\n",
        "    print(word_embeddings_google_news)\n",
        "except Exception as e:\n",
        "    print(\"Failed to load Google News Word2Vec model:\", e)\n",
        "\n",
        "# Method 2: Gensim Word2Vec\n",
        "print(\"\\nGensim Word2Vec:\")\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "# Train Word2Vec model (small example for demonstration)\n",
        "corpus = [words]\n",
        "model_gensim = gensim.models.Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
        "# Generate word embeddings\n",
        "word_embeddings_gensim = []\n",
        "for word in words:\n",
        "    word_embeddings_gensim.append(model_gensim.wv[word])\n",
        "print(word_embeddings_gensim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd2sWOTYBdz1",
        "outputId": "124d06bd-ad9c-4b3a-dd8f-168c86dd4df5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Embedding (Word2Vec): [ 0.02669853 -0.47893864  0.39087495 -0.21142425  0.01161683 -0.92668456\n",
            "  0.0437663   0.9929235   0.05645761 -1.192056    0.16014831 -0.5003815\n",
            " -0.5894984  -0.04715503  0.43917355 -0.33463976  0.09737411 -0.04741639\n",
            " -0.2527418  -1.3808872   0.97781837  0.5785      1.3699572   0.21506524\n",
            " -0.5863373   0.06429586 -0.2800557   0.1117833  -0.48486203  0.36876446\n",
            "  0.8539407   0.07271117  1.9124475  -0.5394428  -0.2082707  -0.82588124\n",
            " -0.34582916  0.17808557  0.02177669 -0.6058211  -0.08510967 -0.5877332\n",
            "  0.6880538   0.38418373  0.81730837 -0.07223422  0.0733491  -0.48579267\n",
            " -0.6614127   0.53780764 -0.528442   -0.6018595  -0.64849067 -0.8216293\n",
            "  0.27421775 -0.8194715   0.16036002 -0.17419407  0.5780222   0.47250655\n",
            "  0.4679924   0.21947333  0.67071486 -0.38005382 -0.66013336  0.94811016\n",
            "  0.65726936  0.9325848  -1.1560645   0.9561688   0.52941763  0.10322586\n",
            "  0.48056915  0.52799976  0.6918715  -0.29918048  0.8921094   0.5703312\n",
            " -0.7902481  -0.07649312  0.05572318  0.16393179 -0.7968085   0.8965455\n",
            " -0.7103489  -0.26378313 -0.12005704  0.5470369  -0.18195985  0.00706323\n",
            "  0.8901511  -0.26533487  0.768998    0.44449148 -0.33223757  0.731739\n",
            "  0.4986168  -0.05926355 -0.04835339  0.2115229 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "nltk.download('tagsets')\n",
        "# Get the POS tagset mapping\n",
        "tagset_mapping = nltk.data.load('help/tagsets/upenn_tagset.pickle')\n",
        "\n",
        "# Print the POS tags and their descriptions\n",
        "for tag, description in tagset_mapping.items():\n",
        "    print(f\"{tag}: {description}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azMmu1ywfzAm",
        "outputId": "ceefe601-de15-43d2-c64b-903a7ad33f8c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LS: ('list item marker', 'A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005 SP-44007 Second Third Three Two * a b c d first five four one six three two ')\n",
            "TO: ('\"to\" as preposition or infinitive marker', 'to ')\n",
            "VBN: ('verb, past participle', 'multihulled dilapidated aerosolized chaired languished panelized used experimented flourished imitated reunifed factored condensed sheared unsettled primed dubbed desired ... ')\n",
            "'': ('closing quotation mark', \"' '' \")\n",
            "WP: ('WH-pronoun', 'that what whatever whatsoever which who whom whosoever ')\n",
            "UH: ('interjection', 'Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly man baby diddle hush sonuvabitch ... ')\n",
            "VBG: ('verb, present participle or gerund', \"telegraphing stirring focusing angering judging stalling lactating hankerin' alleging veering capping approaching traveling besieging encrypting interrupting erasing wincing ... \")\n",
            "JJ: ('adjective or numeral, ordinal', 'third ill-mannered pre-war regrettable oiled calamitous first separable ectoplasmic battery-powered participatory fourth still-to-be-named multilingual multi-disciplinary ... ')\n",
            "VBZ: ('verb, present tense, 3rd person singular', 'bases reconstructs marks mixes displeases seals carps weaves snatches slumps stretches authorizes smolders pictures emerges stockpiles seduces fizzes uses bolsters slaps speaks pleads ... ')\n",
            "--: ('dash', '-- ')\n",
            "VBP: ('verb, present tense, not 3rd person singular', 'predominate wrap resort sue twist spill cure lengthen brush terminate appear tend stray glisten obtain comprise detest tease attract emphasize mold postpone sever return wag ... ')\n",
            "NN: ('noun, common, singular or mass', 'common-carrier cabbage knuckle-duster Casino afghan shed thermostat investment slide humour falloff slick wind hyena override subhumanity machinist ... ')\n",
            "DT: ('determiner', 'all an another any both del each either every half la many much nary neither no some such that the them these this those ')\n",
            "PRP: ('pronoun, personal', 'hers herself him himself hisself it itself me myself one oneself ours ourselves ownself self she thee theirs them themselves they thou thy us ')\n",
            ":: ('colon or ellipsis', ': ; ... ')\n",
            "WP$: ('WH-pronoun, possessive', 'whose ')\n",
            "NNPS: ('noun, proper, plural', 'Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques Apache Apaches Apocrypha ... ')\n",
            "PRP$: ('pronoun, possessive', 'her his mine my our ours their thy your ')\n",
            "WDT: ('WH-determiner', 'that what whatever which whichever ')\n",
            "(: ('opening parenthesis', '( [ { ')\n",
            "): ('closing parenthesis', ') ] } ')\n",
            ".: ('sentence terminator', '. ! ? ')\n",
            ",: ('comma', ', ')\n",
            "``: ('opening quotation mark', '` `` ')\n",
            "$: ('dollar', '$ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$ ')\n",
            "RB: ('adverb', 'occasionally unabatingly maddeningly adventurously professedly stirringly prominently technologically magisterially predominately swiftly fiscally pitilessly ... ')\n",
            "RBR: ('adverb, comparative', 'further gloomier grander graver greater grimmer harder harsher healthier heavier higher however larger later leaner lengthier less-perfectly lesser lonelier longer louder lower more ... ')\n",
            "RBS: ('adverb, superlative', 'best biggest bluntest earliest farthest first furthest hardest heartiest highest largest least less most nearest second tightest worst ')\n",
            "VBD: ('verb, past tense', 'dipped pleaded swiped regummed soaked tidied convened halted registered cushioned exacted snubbed strode aimed adopted belied figgered speculated wore appreciated contemplated ... ')\n",
            "IN: ('preposition or conjunction, subordinating', 'astride among uppon whether out inside pro despite on by throughout below within for towards near behind atop around if like until below next into if beside ... ')\n",
            "FW: ('foreign word', \"gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte terram fiche oui corporis ... \")\n",
            "RP: ('particle', 'aboard about across along apart around aside at away back before behind by crop down ever fast for forth from go high i.e. in into just later low more off on open out over per pie raising start teeth that through under unto up up-pp upon whole with you ')\n",
            "JJR: ('adjective, comparative', 'bleaker braver breezier briefer brighter brisker broader bumper busier calmer cheaper choosier cleaner clearer closer colder commoner costlier cozier creamier crunchier cuter ... ')\n",
            "JJS: ('adjective, superlative', 'calmest cheapest choicest classiest cleanest clearest closest commonest corniest costliest crassest creepiest crudest cutest darkest deadliest dearest deepest densest dinkiest ... ')\n",
            "PDT: ('pre-determiner', 'all both half many quite such sure this ')\n",
            "MD: ('modal auxiliary', \"can cannot could couldn't dare may might must need ought shall should shouldn't will would \")\n",
            "VB: ('verb, base form', 'ask assemble assess assign assume atone attention avoid bake balkanize bank begin behold believe bend benefit bevel beware bless boil bomb boost brace break bring broil brush build ... ')\n",
            "WRB: ('Wh-adverb', 'how however whence whenever where whereby whereever wherein whereof why ')\n",
            "NNP: ('noun, proper, singular', 'Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA Shannon A.K.C. Meltex Liverpool ... ')\n",
            "EX: ('existential there', 'there ')\n",
            "NNS: ('noun, common, plural', 'undergraduates scotches bric-a-brac products bodyguards facets coasts divestitures storehouses designs clubs fragrances averages subjectivists apprehensions muses factory-jobs ... ')\n",
            "SYM: ('symbol', \"% & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** *** \")\n",
            "CC: ('conjunction, coordinating', \"& 'n and both but either et for less minus neither nor or plus so therefore times v. versus vs. whether yet \")\n",
            "CD: ('numeral, cardinal', \"mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025 fifteen 271,124 dozen quintillion DM2,000 ... \")\n",
            "POS: ('genitive marker', \"' 's \")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Initialize the WordNet lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize words\n",
        "words = [\"running\", \"ate\", \"better\"]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(lemmatized_words)\n",
        "\n",
        "\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load English and French models\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
        "\n",
        "# Sample sentences\n",
        "sentences_en = [\n",
        "    \"I am running in the park.\",\n",
        "    \"They are eating pizza.\",\n",
        "    \"The cat is sitting on the mat.\"\n",
        "]\n",
        "\n",
        "sentences_fr = [\n",
        "    \"Je cours dans le parc.\",\n",
        "    \"Ils mangent de la pizza.\",\n",
        "    \"Le chat est assis sur le tapis.\"\n",
        "]\n",
        "\n",
        "# Lemmatize English sentences\n",
        "print(\"English Lemmatization:\")\n",
        "for sentence in sentences_en:\n",
        "    doc = nlp_en(sentence)\n",
        "    lemmatized_words = [token.lemma_ for token in doc]\n",
        "    print(lemmatized_words)\n",
        "\n",
        "# Lemmatize French sentences\n",
        "print(\"\\nFrench Lemmatization:\")\n",
        "for sentence in sentences_fr:\n",
        "    doc = nlp_fr(sentence)\n",
        "    lemmatized_words = [token.lemma_ for token in doc]\n",
        "    print(lemmatized_words)\n"
      ],
      "metadata": {
        "id": "vFxiFlJeBgfM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1dce445e-39dd-481d-fea5-48c92e1ec397"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-ebe7b2dd8ec4>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Lemmatize words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"running\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"better\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlemmatized_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatized_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-ebe7b2dd8ec4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Lemmatize words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"running\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"better\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlemmatized_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatized_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mword\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define a custom pipeline component for lemmatization\n",
        "@spacy.Language.component(\"custom_lemmatization\")\n",
        "def custom_lemmatization(doc):\n",
        "    for token in doc:\n",
        "        if token.pos_ in [\"VERB\", \"ADJ\", \"ADV\", \"NOUN\"]:\n",
        "            token.lemma_ = token.lemma_.lower()\n",
        "    return doc\n",
        "\n",
        "# Add the custom component to the pipeline\n",
        "nlp.add_pipe(\"custom_lemmatization\", last=True)\n",
        "\n",
        "# Print the updated pipeline\n",
        "print(\"Updated NLP Pipeline:\")\n",
        "for name, component in nlp.pipeline:\n",
        "    print(f\"{name}: {component}\")\n"
      ],
      "metadata": {
        "id": "yN6aHwL4BiCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518fe611-7cae-4c86-9218-0d0a9944d38c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated NLP Pipeline:\n",
            "tok2vec: <spacy.pipeline.tok2vec.Tok2Vec object at 0x7dfd23a42ce0>\n",
            "tagger: <spacy.pipeline.tagger.Tagger object at 0x7dfd23a430a0>\n",
            "parser: <spacy.pipeline.dep_parser.DependencyParser object at 0x7dfd239efca0>\n",
            "attribute_ruler: <spacy.pipeline.attributeruler.AttributeRuler object at 0x7dfd230fe700>\n",
            "lemmatizer: <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7dfd2376cb00>\n",
            "ner: <spacy.pipeline.ner.EntityRecognizer object at 0x7dfd239efc30>\n",
            "custom_lemmatization: <function custom_lemmatization at 0x7dfd28cdb760>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4rafEyRyUfH3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}