# -*- coding: utf-8 -*-
"""all_nlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uLE5nQZiV_YpptOKoS6MKXyi38ftzNUl
"""

!pip install langdetect
!pip install textblob

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from langdetect import detect
from collections import Counter

# Download necessary resources for NLTK
nltk.download('punkt')

# Load the news dataset
def load_dataset(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        headlines = []
        for line in file:
            headlines.append(line.strip())
    return headlines

# Function to detect languages of headlines
def language_detection(headlines):
    languages = []
    for headline in headlines:
        detected_language = detect(headline)
        languages.append(detected_language)
    return languages

# Function to count total words in headlines
def word_count(headlines):
    total_words = 0
    for headline in headlines:
        words = word_tokenize(headline)
        total_words += len(words)
    return total_words

# Function to count total sentences in headlines
def sentence_count(headlines):
    total_sentences = 0
    for headline in headlines:
        sentences = sent_tokenize(headline)
        total_sentences += len(sentences)
    return total_sentences

# Function to tokenize words in headlines
def word_tokenization(headlines):
    words_tokenized = []
    for headline in headlines:
        words = word_tokenize(headline)
        words_tokenized.append(words)
    return words_tokenized

if __name__ == "__main__":
    # file_path = "news_dataset.txt"
    # headlines = load_dataset(file_path)
    headlines = []
    # Language detection
    detected_languages = language_detection(headlines)
    print("Detected Languages:", detected_languages[:5])  # Displaying first 5 detected languages

    # Word count
    total_words = word_count(headlines)
    print("Total Words:", total_words)

    # Sentence count
    total_sentences = sentence_count(headlines)
    print("Total Sentences:", total_sentences)

    # Word tokenization
    words_tokenized = word_tokenization(headlines)
    print("Words Tokenized:", words_tokenized[:2])  # Displaying tokenized words of first 2 headlines



import nltk
from nltk.tokenize import word_tokenize, sent_tokenize, WhitespaceTokenizer, RegexpTokenizer, TweetTokenizer, PunktSentenceTokenizer, WordPunctTokenizer, TreebankWordTokenizer

# Sample text for demonstration
text = "NLTK is a powerful library for natural language processing. It provides various tokenizers for different tasks, such as word tokenization and sentence tokenization. Tweet @NLTKOrg for more info! #NLTK #NLP"

# Word Tokenizer
words_word_tokenize = word_tokenize(text)
print("Word Tokenizer:", words_word_tokenize)

# Sentence Tokenizer
sentences_sent_tokenize = sent_tokenize(text)
print("Sentence Tokenizer:", sentences_sent_tokenize)

# Whitespace Tokenizer
tokenizer_whitespace = WhitespaceTokenizer()
words_whitespace_tokenize = tokenizer_whitespace.tokenize(text)
print("Whitespace Tokenizer:", words_whitespace_tokenize)

# Regexp Tokenizer
tokenizer_regexp = RegexpTokenizer(r'\w+')
words_regexp_tokenize = tokenizer_regexp.tokenize(text)
print("Regexp Tokenizer:", words_regexp_tokenize)

# WordPunct Tokenizer
tokenizer_wordpunct = WordPunctTokenizer()
words_wordpunct_tokenize = tokenizer_wordpunct.tokenize(text)
print("WordPunct Tokenizer:", words_wordpunct_tokenize)

import nltk
from nltk.stem import PorterStemmer, LancasterStemmer
from nltk.stem import SnowballStemmer

# Sample words for stemming
words = ["running", "ran", "runs", "runner", "easily", "fairly", "fairness"]

# Porter Stemmer
porter_stemmer = PorterStemmer()
stemmed_words_porter = [porter_stemmer.stem(word) for word in words]
print("Porter Stemmer:", stemmed_words_porter)

# Lancaster Stemmer
lancaster_stemmer = LancasterStemmer()
stemmed_words_lancaster = [lancaster_stemmer.stem(word) for word in words]
print("Lancaster Stemmer:", stemmed_words_lancaster)


# Snowball Stemmer for English
snowball_stemmer = SnowballStemmer("english")
stemmed_words_snowball = [snowball_stemmer.stem(word) for word in words]
print("Snowball Stemmer:", stemmed_words_snowball)

import spacy
from collections import Counter

# Load English tokenizer, tagger, parser, NER, and word vectors
nlp = spacy.load("en_core_web_sm")

# Sample text for demonstration
text = """
Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.
"""

# Process the text with spaCy
doc = nlp(text)

# Perform POS tagging and count POS tags
pos_tags = Counter()
for token in doc:
    pos_tags[token.pos_] += 1

# Print frequency list of POS tags
print("Frequency of POS tags:")
for tag, count in pos_tags.items():
    print(f"{tag}: {count}")

from textblob import TextBlob
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize

# Download NLTK resources
nltk.download('punkt')
nltk.download('vader_lexicon')

# Sample text for demonstration
text = "I love this movie! It's so uplifting and heartwarming. However, the ending made me sad."

# Sentiment analysis using TextBlob
blob = TextBlob(text)
sentiment = blob.sentiment.polarity
sentiment_label = "positive" if sentiment > 0 else "negative"

print("Sentiment (TextBlob):", sentiment_label)

# Emotion detection using NLTK
sia = SentimentIntensityAnalyzer()
tokens = word_tokenize(text)
emotion_scores = sia.polarity_scores(text)

# Determine emotion based on the sentiment scores
emotion_label = None
if emotion_scores['pos'] > emotion_scores['neg']:
    emotion_label = "positive"
elif emotion_scores['pos'] < emotion_scores['neg']:
    emotion_label = "negative"
else:
    emotion_label = "neutral"

print("Emotion (NLTK):", emotion_label)

import nltk
from nltk.util import ngrams
from collections import Counter

# Sample text for demonstration
text = "I love natural language processing. It is fascinating."

# Tokenize the text into words
words = nltk.word_tokenize(text)

# Define the order of the N-gram model (N)
N = 2

# Generate N-grams from the tokenized words
ngrams_list = list(ngrams(words, N))

# Count occurrences of each N-gram
ngram_counts = Counter(ngrams_list)

# Calculate the probability of a sequence of words using N-gram model
def calculate_probability(sequence):
    if len(sequence) != N:
        raise ValueError("Length of sequence must be equal to N")

    # Convert the sequence to tuple for comparison with N-grams
    sequence_tuple = tuple(sequence)

    # Count occurrences of the sequence in the N-grams
    sequence_count = ngram_counts.get(sequence_tuple, 0)

    # Count occurrences of the first (N-1) words in the sequence
    prefix_count = sum(1 for gram in ngrams_list if gram[:-1] == sequence_tuple[:-1])

    # Calculate the probability using Laplace smoothing (add-one smoothing)
    probability = (sequence_count + 1) / (prefix_count + len(set(ngrams_list)))

    return probability

# Example: Calculate the probability of a sequence "natural language"
sequence = ["natural", "language"]
probability = calculate_probability(sequence)
print(f"Probability of sequence '{' '.join(sequence)}': {probability}")

import spacy
from spacy import displacy

# Load the English language model
nlp = spacy.load("en_core_web_sm")

# Sample text for NER
text = """
Apple was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976.
It is headquartered in Cupertino, California, United States.
Apple designs, manufactures, and sells consumer electronics, computer software,
and online services. Its best-known hardware products include the iPhone,
iPad, Mac, and Apple Watch.
"""

# Process the text with spaCy
doc = nlp(text)

# Extract named entities
named_entities = [(ent.text, ent.label_) for ent in doc.ents]

# Display named entities
print("Named Entities:")
for entity, label in named_entities:
    print(f"{entity} - {label}")

# Visualize named entities
displacy.render(doc, style="ent", jupyter=True)

import pandas as pd
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Load the IMDb movie reviews dataset
data = pd.read_csv("imdb_reviews.csv")

# Data Preprocessing
def preprocess_text(text):
    # Remove punctuation, special characters, and numbers
    text = re.sub(r"[^a-zA-Z\s]", "", text)
    # Convert text to lowercase
    text = text.lower()
    # Tokenize the text into words
    words = word_tokenize(text)
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]
    return ' '.join(words)

data['cleaned_review'] = data['review'].apply(preprocess_text)

# Feature Extraction
X = data['cleaned_review']  # Input features (cleaned movie reviews)
y = data['label']   # Target variable (sentiment label)

vectorizer = TfidfVectorizer(max_features=5000)  # Convert text data into numerical representations
X = vectorizer.fit_transform(X)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Building
model = LogisticRegression(max_iter=1000)  # Logistic Regression model
model.fit(X_train, y_train)  # Train the model on the training set

# Model Evaluation
y_pred = model.predict(X_test)  # Predict the labels for the testing set
accuracy = accuracy_score(y_test, y_pred)  # Compute the accuracy of the model
print("Accuracy:", accuracy)

import pandas as pd
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Load the dataset containing spam and ham messages
data = pd.read_csv("spam_ham_messages.csv")

# Data Preprocessing
def preprocess_text(text):
    # Remove punctuation, special characters, and numbers
    text = re.sub(r"[^a-zA-Z\s]", "", text)
    # Convert text to lowercase
    text = text.lower()
    # Tokenize the text into words
    words = word_tokenize(text)
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]
    return ' '.join(words)

data['cleaned_message'] = data['message'].apply(preprocess_text)

# Feature Extraction
X = data['cleaned_message']  # Input features (cleaned messages)
y = data['label']   # Target variable (spam or ham)

vectorizer = TfidfVectorizer(max_features=5000)  # Convert text data into numerical representations
X = vectorizer.fit_transform(X)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Building
model = LogisticRegression(max_iter=1000)  # Logistic Regression model
model.fit(X_train, y_train)  # Train the model on the training set

# Model Evaluation
y_pred = model.predict(X_test)  # Predict the labels for the testing set
accuracy = accuracy_score(y_test, y_pred)  # Compute the accuracy of the model
print("Accuracy:", accuracy)

import nltk

# Train an HMM POS tagger on the Brown corpus
nltk.download('brown')
brown_corpus = nltk.corpus.brown.tagged_sents()
hmm_tagger = nltk.HiddenMarkovModelTagger.train(brown_corpus)

# Test the tagger on a sample sentence
sentence = "The quick brown fox jumps over the lazy dog"
tagged_sentence = hmm_tagger.tag(nltk.word_tokenize(sentence))
print(tagged_sentence)

import gensim.downloader as api
import gensim
import nltk
from nltk.tokenize import word_tokenize

# Download NLTK resources
nltk.download('punkt')

# Sample text for word embeddings
text = "I love natural language processing. It is fascinating."

# Method 1: Google News Word2Vec
print("Google News Word2Vec:")
try:
    # Load Google News Word2Vec model (large model, download may take some time)
    model_google_news = api.load('word2vec-google-news-300')
    # Tokenize the text
    words = word_tokenize(text)
    # Filter out words not in vocabulary
    words_in_vocab = []
    for word in words:
        if word in model_google_news.vocab:
            words_in_vocab.append(word)
    # Generate word embeddings
    word_embeddings_google_news = []
    for word in words_in_vocab:
        word_embeddings_google_news.append(model_google_news[word])
    print(word_embeddings_google_news)
except Exception as e:
    print("Failed to load Google News Word2Vec model:", e)

# Method 2: Gensim Word2Vec
print("\nGensim Word2Vec:")
# Tokenize the text
words = word_tokenize(text)
# Train Word2Vec model (small example for demonstration)
corpus = [words]
model_gensim = gensim.models.Word2Vec(corpus, vector_size=100, window=5, min_count=1, workers=4)
# Generate word embeddings
word_embeddings_gensim = []
for word in words:
    word_embeddings_gensim.append(model_gensim.wv[word])
print(word_embeddings_gensim)

import nltk
nltk.download('treebank')
nltk.download('tagsets')
# Get the POS tagset mapping
tagset_mapping = nltk.data.load('help/tagsets/upenn_tagset.pickle')

# Print the POS tags and their descriptions
for tag, description in tagset_mapping.items():
    print(f"{tag}: {description}")

from nltk.stem import WordNetLemmatizer

# Initialize the WordNet lemmatizer
lemmatizer = WordNetLemmatizer()

# Lemmatize words
words = ["running", "ate", "better"]
lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
print(lemmatized_words)



import spacy

# Load English and French models
nlp_en = spacy.load("en_core_web_sm")
nlp_fr = spacy.load("fr_core_news_sm")

# Sample sentences
sentences_en = [
    "I am running in the park.",
    "They are eating pizza.",
    "The cat is sitting on the mat."
]

sentences_fr = [
    "Je cours dans le parc.",
    "Ils mangent de la pizza.",
    "Le chat est assis sur le tapis."
]

# Lemmatize English sentences
print("English Lemmatization:")
for sentence in sentences_en:
    doc = nlp_en(sentence)
    lemmatized_words = [token.lemma_ for token in doc]
    print(lemmatized_words)

# Lemmatize French sentences
print("\nFrench Lemmatization:")
for sentence in sentences_fr:
    doc = nlp_fr(sentence)
    lemmatized_words = [token.lemma_ for token in doc]
    print(lemmatized_words)

import spacy

# Load English model
nlp = spacy.load("en_core_web_sm")

# Define a custom pipeline component for lemmatization
@spacy.Language.component("custom_lemmatization")
def custom_lemmatization(doc):
    for token in doc:
        if token.pos_ in ["VERB", "ADJ", "ADV", "NOUN"]:
            token.lemma_ = token.lemma_.lower()
    return doc

# Add the custom component to the pipeline
nlp.add_pipe("custom_lemmatization", last=True)

# Print the updated pipeline
print("Updated NLP Pipeline:")
for name, component in nlp.pipeline:
    print(f"{name}: {component}")

