# -*- coding: utf-8 -*-
"""11_lemmatization .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SFDqaO2JN8apZqALVUcxOTydLbsayVGS
"""

from nltk.stem import WordNetLemmatizer

# Initialize the WordNet lemmatizer
lemmatizer = WordNetLemmatizer()

# Lemmatize words
words = ["running", "ate", "better"]
lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
print(lemmatized_words)



import spacy

# Load English and French models
nlp_en = spacy.load("en_core_web_sm")
nlp_fr = spacy.load("fr_core_news_sm")

# Sample sentences
sentences_en = [
    "I am running in the park.",
    "They are eating pizza.",
    "The cat is sitting on the mat."
]

sentences_fr = [
    "Je cours dans le parc.",
    "Ils mangent de la pizza.",
    "Le chat est assis sur le tapis."
]

# Lemmatize English sentences
print("English Lemmatization:")
for sentence in sentences_en:
    doc = nlp_en(sentence)
    lemmatized_words = [token.lemma_ for token in doc]
    print(lemmatized_words)

# Lemmatize French sentences
print("\nFrench Lemmatization:")
for sentence in sentences_fr:
    doc = nlp_fr(sentence)
    lemmatized_words = [token.lemma_ for token in doc]
    print(lemmatized_words)

import spacy

# Load English model
nlp = spacy.load("en_core_web_sm")

# Define a custom pipeline component for lemmatization
@spacy.Language.component("custom_lemmatization")
def custom_lemmatization(doc):
    for token in doc:
        if token.pos_ in ["VERB", "ADJ", "ADV", "NOUN"]:
            token.lemma_ = token.lemma_.lower()
    return doc

# Add the custom component to the pipeline
nlp.add_pipe("custom_lemmatization", last=True)

# Print the updated pipeline
print("Updated NLP Pipeline:")
for name, component in nlp.pipeline:
    print(f"{name}: {component}")