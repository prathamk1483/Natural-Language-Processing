# -*- coding: utf-8 -*-
"""5_ngram .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SFDqaO2JN8apZqALVUcxOTydLbsayVGS
"""

import nltk
from nltk.util import ngrams
from collections import Counter

# Sample text for demonstration
text = "I love natural language processing. It is fascinating."

# Tokenize the text into words
words = nltk.word_tokenize(text)

# Define the order of the N-gram model (N)
N = 2

# Generate N-grams from the tokenized words
ngrams_list = list(ngrams(words, N))

# Count occurrences of each N-gram
ngram_counts = Counter(ngrams_list)

# Calculate the probability of a sequence of words using N-gram model
def calculate_probability(sequence):
    if len(sequence) != N:
        raise ValueError("Length of sequence must be equal to N")

    # Convert the sequence to tuple for comparison with N-grams
    sequence_tuple = tuple(sequence)

    # Count occurrences of the sequence in the N-grams
    sequence_count = ngram_counts.get(sequence_tuple, 0)

    # Count occurrences of the first (N-1) words in the sequence
    prefix_count = sum(1 for gram in ngrams_list if gram[:-1] == sequence_tuple[:-1])

    # Calculate the probability using Laplace smoothing (add-one smoothing)
    probability = (sequence_count + 1) / (prefix_count + len(set(ngrams_list)))

    return probability

# Example: Calculate the probability of a sequence "natural language"
sequence = ["natural", "language"]
probability = calculate_probability(sequence)
print(f"Probability of sequence '{' '.join(sequence)}': {probability}")