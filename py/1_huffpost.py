# -*- coding: utf-8 -*-
"""Untitled25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SFDqaO2JN8apZqALVUcxOTydLbsayVGS
"""

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from langdetect import detect
from collections import Counter

# Download necessary resources for NLTK
nltk.download('punkt')

# Load the news dataset
def load_dataset(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        headlines = []
        for line in file:
            headlines.append(line.strip())
    return headlines

# Function to detect languages of headlines
def language_detection(headlines):
    languages = []
    for headline in headlines:
        detected_language = detect(headline)
        languages.append(detected_language)
    return languages

# Function to count total words in headlines
def word_count(headlines):
    total_words = 0
    for headline in headlines:
        words = word_tokenize(headline)
        total_words += len(words)
    return total_words

# Function to count total sentences in headlines
def sentence_count(headlines):
    total_sentences = 0
    for headline in headlines:
        sentences = sent_tokenize(headline)
        total_sentences += len(sentences)
    return total_sentences

# Function to tokenize words in headlines
def word_tokenization(headlines):
    words_tokenized = []
    for headline in headlines:
        words = word_tokenize(headline)
        words_tokenized.append(words)
    return words_tokenized

if __name__ == "__main__":
    # file_path = "news_dataset.txt"
    # headlines = load_dataset(file_path)
    headlines = []
    # Language detection
    detected_languages = language_detection(headlines)
    print("Detected Languages:", detected_languages[:5])  # Displaying first 5 detected languages

    # Word count
    total_words = word_count(headlines)
    print("Total Words:", total_words)

    # Sentence count
    total_sentences = sentence_count(headlines)
    print("Total Sentences:", total_sentences)

    # Word tokenization
    words_tokenized = word_tokenization(headlines)
    print("Words Tokenized:", words_tokenized[:2])  # Displaying tokenized words of first 2 headlines



import nltk
from nltk.tokenize import word_tokenize, sent_tokenize, WhitespaceTokenizer, RegexpTokenizer, TweetTokenizer, PunktSentenceTokenizer, WordPunctTokenizer, TreebankWordTokenizer

# Sample text for demonstration
text = "NLTK is a powerful library for natural language processing. It provides various tokenizers for different tasks, such as word tokenization and sentence tokenization. Tweet @NLTKOrg for more info! #NLTK #NLP"

# Word Tokenizer
words_word_tokenize = word_tokenize(text)
print("Word Tokenizer:", words_word_tokenize)

# Sentence Tokenizer
sentences_sent_tokenize = sent_tokenize(text)
print("Sentence Tokenizer:", sentences_sent_tokenize)

# Whitespace Tokenizer
tokenizer_whitespace = WhitespaceTokenizer()
words_whitespace_tokenize = tokenizer_whitespace.tokenize(text)
print("Whitespace Tokenizer:", words_whitespace_tokenize)

# Regexp Tokenizer
tokenizer_regexp = RegexpTokenizer(r'\w+')
words_regexp_tokenize = tokenizer_regexp.tokenize(text)
print("Regexp Tokenizer:", words_regexp_tokenize)

# WordPunct Tokenizer
tokenizer_wordpunct = WordPunctTokenizer()
words_wordpunct_tokenize = tokenizer_wordpunct.tokenize(text)
print("WordPunct Tokenizer:", words_wordpunct_tokenize)